{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤–ğŸ’¬ Chatbot inteligente para geraÃ§Ã£o de respostas em linguagem pt-br\n",
    "\n",
    "Este projeto utiliza o modelo **PTT5 (Portuguese T5)**, uma adaptaÃ§Ã£o do modelo **T5 (Text-to-Text Transfer Transformer)** para a lÃ­ngua portuguesa.\n",
    "\n",
    "- ğŸ“ **Arquitetura:** baseada no Transformer, com codificador e decodificador (encoder-decoder).\n",
    "- ğŸ§  **Capacidade:** compreende instruÃ§Ãµes em linguagem natural e gera respostas coerentes e contextualizadas.\n",
    "- ğŸ“š **Aprendizado:** usa fine-tuning sobre dados em portuguÃªs via aprendizado por transferÃªncia.\n",
    "- ğŸ› ï¸ **Base:** modelo `unicamp-dl/ptt5-base-portuguese-vocab`.\n",
    "\n",
    "\n",
    "> âš ï¸ dataset.csv, requirements.txt incluÃ­das no repositÃ³rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§° Parte 1: Ambiente e dependÃªncias\n",
    "\n",
    "Nesta seÃ§Ã£o, definimos o ambiente necessÃ¡rio para executar o projeto, garantindo que todas as bibliotecas estejam corretamente instaladas e compatÃ­veis com o modelo utilizado.\n",
    "\n",
    "As etapas abaixo cobrem:\n",
    "\n",
    "- EspecificaÃ§Ã£o da versÃ£o do Python e sistema operacional\n",
    "- OrganizaÃ§Ã£o do ambiente virtual (Conda recomendado)\n",
    "- InstalaÃ§Ã£o das dependÃªncias via `pip` ou `requirements.txt`\n",
    "- Registro das bibliotecas utilizadas no desenvolvimento\n",
    "\n",
    "> âš ï¸ Ter um ambiente reprodutÃ­vel Ã© essencial para evitar conflitos de versÃ£o e garantir que o modelo funcione como esperado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš™ï¸ 1.1: Ambiente e dependÃªncias utilizadas\n",
    "\n",
    "O modelo foi treinado e executado em um ambiente com a seguinte configuraÃ§Ã£o:\n",
    "\n",
    "- ğŸ **Python**: 3.9.x (via Anaconda) \n",
    "- ğŸ’» **Sistema Operacional**: Windows (compatÃ­vel tambÃ©m com Linux)  \n",
    "- ğŸ§ª **Ambiente virtual**: criado com o **Conda**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“¦ 1.2: Bibliotecas essenciais\n",
    "\n",
    "!pip install transformers==4.37.2\n",
    "!pip install datasets==2.16.1\n",
    "!pip install pandas==2.1.4\n",
    "!pip install torch==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ—ƒï¸ 1.3: InstalaÃ§Ã£o das dependÃªncias\n",
    "\n",
    "Caso queira utilizar todas as bibliotecas que foram utilizadas utilize o arquivo `requirements.txt`. Para instalar, ative o ambiente Conda desejado e execute:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‹ï¸â€â™‚ï¸ Parte 2: Treinamento do modelo PTT5 com dados personalizados\n",
    "\n",
    "Nesta seÃ§Ã£o, executamos todas as etapas relacionadas ao **processo de fine-tuning do modelo PTT5** em portuguÃªs, utilizando um dataset customizado.\n",
    "\n",
    "As etapas abaixo cobrem:\n",
    "\n",
    "- ImportaÃ§Ã£o de bibliotecas\n",
    "- Carregamento e preparaÃ§Ã£o do modelo\n",
    "- Processamento e tokenizaÃ§Ã£o dos dados\n",
    "- DefiniÃ§Ã£o de argumentos de treinamento\n",
    "- ExecuÃ§Ã£o do treinamento com monitoramento de loss\n",
    "- Salvamento do modelo final e uso para inferÃªncia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’» Etapa 2.1: ImportaÃ§Ã£o das bibliotecas\n",
    "\n",
    "        Importamos os mÃ³dulos essenciais para manipular dados, preparar o modelo e realizar o treinamento com Transformers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“˜ Etapa 2.2: Callback para monitorar a perda por Ã©poca\n",
    "\n",
    "        Esta funÃ§Ã£o imprime a loss ao final de cada Ã©poca durante o treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLossPrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\nğŸ“˜ [Ã‰poca {int(state.epoch)}] Loss de Treinamento: {state.log_history[-1]['loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“¥ Etapa 2.3: Carregando o modelo PTT5 e o tokenizer\n",
    "\n",
    "        Utilizamos a versÃ£o base do modelo T5 treinado em portuguÃªs pela Unicamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unicamp-dl/ptt5-base-portuguese-vocab\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“¥ Etapa 2.4: Carregando e preparando o dataset CSV\n",
    "\n",
    "        Unificamos as colunas do dataset em uma entrada textual formatada para o modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df[\"input_text\"] = \"tÃ³pico: \" + df[\"topic\"] + \" | instruÃ§Ã£o: \" + df[\"instruction\"]\n",
    "df[\"target_text\"] = df[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“š Etapa 2.5: TransformaÃ§Ã£o do DataFrame em um objeto Dataset \n",
    "\n",
    "        Transformamos o DataFrame em um objeto Dataset para integraÃ§Ã£o com o `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df[[\"input_text\", \"target_text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§© Etapa 2.6: TokenizaÃ§Ã£o dos dados\n",
    "\n",
    "        Aplicamos truncamento, padding e preparamos os pares entrada/saÃ­da para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    input_enc = tokenizer(batch[\"input_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    target_enc = tokenizer(batch[\"target_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“˜ Etapa 2.7: ConfiguraÃ§Ã£o dos hiperparÃ¢metros de treinamento\n",
    "\n",
    "        Definimos estratÃ©gia de salvamento, taxa de aprendizado, uso de FP16 e nÃºmero de Ã©pocas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ptt5-modelo-treinado\",       # Pasta onde os checkpoints do modelo serÃ£o salvos\n",
    "    overwrite_output_dir=True,                 # Sobrescreve a pasta de saÃ­da se ela jÃ¡ existir\n",
    "    per_device_train_batch_size=30,            # Tamanho do batch por dispositivo (ex: GPU) durante o treinamento\n",
    "    gradient_accumulation_steps=2,             # Acumula gradientes por 2 batches antes de atualizar os pesos (simula batch_size=60)\n",
    "    learning_rate=3e-4,                        # Taxa de aprendizado inicial (valor relativamente alto, ideal para fine-tuning)\n",
    "    num_train_epochs=30,                       # NÃºmero total de Ã©pocas de treinamento (passadas completas pelo dataset)\n",
    "    save_strategy=\"epoch\",                     # Salva um checkpoint do modelo ao final de cada Ã©poca\n",
    "    weight_decay=0.005,                        # Taxa de decaimento dos pesos (regularizaÃ§Ã£o para evitar overfitting)\n",
    "    warmup_steps=100,                          # NÃºmero de steps com aprendizado mais suave no inÃ­cio (warm-up)\n",
    "    logging_steps=10,                          # FrequÃªncia (em steps) com que as mÃ©tricas de treino serÃ£o logadas no console\n",
    "    fp16=torch.cuda.is_available(),            # Ativa treinamento em precisÃ£o mista (FP16) se houver GPU compatÃ­vel (mais rÃ¡pido e leve)\n",
    "    report_to=\"none\",                          # Desativa integraÃ§Ã£o com sistemas de logging externos (ex: TensorBoard, WandB)\n",
    "    save_total_limit=3                         # MantÃ©m no mÃ¡ximo 3 checkpoints salvos no disco (os mais recentes)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš™ï¸ Etapa 2.8: InicializaÃ§Ã£o do Trainer\n",
    "\n",
    "        Instanciamos o `Trainer` com o modelo, os dados tokenizados e o callback para logging da loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                               # Modelo PTT5 carregado e pronto para ser treinado\n",
    "    args=training_args,                        # Conjunto de argumentos de treinamento definidos anteriormente (TrainingArguments)\n",
    "    train_dataset=tokenized_dataset,           # Dataset jÃ¡ tokenizado que serÃ¡ usado para o treinamento\n",
    "    callbacks=[EpochLossPrinterCallback()]     # Lista de callbacks personalizados; neste caso, imprime a loss ao final de cada Ã©poca\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸš€ Etapa 2.9: InÃ­cio do treinamento\n",
    "\n",
    "        Executamos o fine-tuning do modelo com os dados fornecidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¾ Etapa 2.10: Salvando o modelo e o tokenizer treinados\n",
    "\n",
    "        ApÃ³s o treinamento, salvamos o modelo fine-tunado e seu tokenizer para uso posterior em inferÃªncia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./ptt5-modelo-final\")\n",
    "tokenizer.save_pretrained(\"./ptt5-modelo-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¬ Parte 3: Uso do modelo treinado para geraÃ§Ã£o de respostas\n",
    "\n",
    "Com o modelo PTT5 jÃ¡ treinado e salvo, esta seÃ§Ã£o aborda as etapas necessÃ¡rias para utilizÃ¡-lo em tempo de execuÃ§Ã£o.\n",
    "\n",
    "Aqui, veremos:\n",
    "\n",
    "- Como **carregar o modelo salvo** e seu tokenizer\n",
    "- Definir uma funÃ§Ã£o `responder()` que gere respostas a partir de perguntas em linguagem natural\n",
    "- Realizar **testes prÃ¡ticos com perguntas reais** simulando um chatbot\n",
    "- Explorar diferentes configuraÃ§Ãµes de geraÃ§Ã£o, como `temperature`, `top_k` e `top_p`\n",
    "\n",
    "> ğŸ“Œ Esta seÃ§Ã£o Ã© fundamental para validar a performance prÃ¡tica do modelo apÃ³s o fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ” Etapa 3.1: Carregando o modelo final para uso\n",
    "\n",
    "        Agora vamos utilizar o modelo PTT5 fine-tunado para responder perguntas reais em portuguÃªs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Caminho para o modelo salvo\n",
    "model_dir = \"./ptt5-modelo-final\"\n",
    "\n",
    "# Carregando o tokenizer e o modelo\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“˜ Etapa 3.2: FunÃ§Ã£o para geraÃ§Ã£o de respostas\n",
    "\n",
    "        Criamos uma funÃ§Ã£o `responder()` que recebe uma instruÃ§Ã£o e retorna uma resposta gerada pelo modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def responder(pergunta):\n",
    "    # Concatena o prefixo \"instruÃ§Ã£o: \" com a pergunta do usuÃ¡rio (formato usado no treinamento do modelo)\n",
    "    entrada = \"instruÃ§Ã£o: \" + pergunta\n",
    "\n",
    "    # Tokeniza a entrada textual para o formato aceito pelo modelo\n",
    "    inputs = tokenizer(\n",
    "        entrada,                 # Texto de entrada para o modelo\n",
    "        return_tensors=\"pt\",     # Retorna tensores do PyTorch\n",
    "        truncation=True,         # Trunca o texto se ultrapassar o limite mÃ¡ximo\n",
    "        padding=True,            # Aplica padding para atingir o tamanho fixo\n",
    "        max_length=128           # Define o tamanho mÃ¡ximo da sequÃªncia de entrada\n",
    "    )\n",
    "\n",
    "    # Gera a saÃ­da textual usando o modelo treinado\n",
    "    outputs = model.generate(\n",
    "        **inputs,                # Passa os tensores tokenizados como entrada\n",
    "        max_length=64,           # Limita o tamanho da resposta gerada\n",
    "        do_sample=True,          # Ativa amostragem aleatÃ³ria (mais criativo)\n",
    "        top_k=50,                # Considera apenas os 50 tokens mais provÃ¡veis (Top-K sampling)\n",
    "        top_p=0.95,              # Aplica nucleus sampling (Top-P), acumulando atÃ© 95% de probabilidade\n",
    "        temperature=0.7,         # Controla a aleatoriedade (quanto menor, mais conservador)\n",
    "        repetition_penalty=1.2,  # Penaliza repetiÃ§Ãµes para evitar respostas redundantes\n",
    "        num_return_sequences=1   # Gera apenas uma resposta\n",
    "    )\n",
    "\n",
    "    # Decodifica a resposta gerada de volta para string legÃ­vel (removendo tokens especiais)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¬ Etapa 3.3: Fazendo perguntas ao modelo treinado\n",
    "        \n",
    "        Abaixo, alguns exemplos reais testando o modelo com instruÃ§Ãµes tÃ­picas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responder(\"A mÃ¡quina aceita cartÃ£o?\"))\n",
    "print(responder(\"Tem coca-cola?\"))\n",
    "print(responder(\"Se um turista usar a mÃ¡quina, ele vai conseguir entender?\"))\n",
    "print(responder(\"Posso pagar com boleto?\"))\n",
    "print(responder(\"Quais sÃ£o a forma de pagamento?\"))\n",
    "print(responder(\"Tem bebida com gÃ¡s?\"))\n",
    "print(responder(\"Tem opÃ§Ã£o de refrigerante sem cafeÃ­na?\"))\n",
    "print(responder(\"O refrigerante nÃ£o sai da mÃ¡quina, como devo proceder?\"))\n",
    "print(responder(\"Qual telefone de suporte?\"))\n",
    "print(responder(\"VocÃª tem laranja?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—‚ï¸ Parte 4: ConstruÃ§Ã£o do dataset\n",
    "\n",
    "Nesta seÃ§Ã£o, explicamos como foi construÃ­do o dataset utilizado no fine-tuning do modelo PTT5, focado na geraÃ§Ã£o de respostas em portuguÃªs com base em instruÃ§Ãµes especÃ­ficas.\n",
    "\n",
    "As etapas abaixo cobrem:\n",
    "\n",
    "- Estrutura das colunas (`topic`, `instruction`, `output`)\n",
    "- EstratÃ©gia de formulaÃ§Ã£o das instruÃ§Ãµes e respostas\n",
    "- GeraÃ§Ã£o e organizaÃ§Ã£o dos exemplos em CSV\n",
    "- Objetivo dos dados: simular interaÃ§Ãµes reais com um chatbot\n",
    "\n",
    "> ğŸ§  A qualidade e variedade do dataset sÃ£o fatores decisivos para o desempenho do modelo. Neste projeto, priorizamos instruÃ§Ãµes curtas, diretas e contextualizadas, refletindo situaÃ§Ãµes reais de atendimento automatizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¯ 4.1: EstratÃ©gia de construÃ§Ã£o dos exemplos\n",
    "\n",
    "> A partir de 5 perguntas humanas por tÃ³pico, vocÃª poderÃ¡ gerar as 100 por tÃ³pico\n",
    "\n",
    "Para garantir a robustez e a naturalidade das respostas geradas pelo modelo, cada tÃ³pico do dataset foi estruturado com 100 exemplos balanceados em quatro categorias distintas:\n",
    "\n",
    "- **30 perguntas variadas com estrutura direta**, explorando diferentes formas de expressar a mesma intenÃ§Ã£o;\n",
    "- **20 perguntas com variaÃ§Ãµes de estilo e contexto**, incluindo registros formais, informais e construÃ§Ãµes regionais;\n",
    "- **30 casos de erro, exceÃ§Ã£o ou negativa**, representando limitaÃ§Ãµes reais do sistema (ex: produto indisponÃ­vel, recurso inexistente);\n",
    "- **20 perguntas indiretas, curiosidades ou detalhes contextuais**, que exigem inferÃªncia semÃ¢ntica ou compreensÃ£o mais sutil.\n",
    "\n",
    "O conjunto total abrange **11 tÃ³picos especÃ­ficos** relacionados ao funcionamento da mÃ¡quina de venda: `sabores`, `acessibilidade`, `horÃ¡rio de funcionamento`, `idioma`, `promoÃ§Ãµes`, `contato`, `reembolso`, `falhas`, `uso da mÃ¡quina`, `produtos` e `pagamento`.\n",
    "\n",
    "Essa abordagem visa simular interaÃ§Ãµes autÃªnticas com usuÃ¡rios em cenÃ¡rios reais, aumentando a capacidade do modelo de generalizar, recusar adequadamente e manter coerÃªncia mesmo diante de instruÃ§Ãµes incomuns ou incompletas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ 4.1.1 EstratÃ©gia para gerar 30 perguntas variadas com estrutura diferente\n",
    "\n",
    "ğŸ¯ Objetivo: Gerar variaÃ§Ãµes de forma (estrutura gramatical) mantendo a mesma intenÃ§Ã£o da pergunta.\n",
    "\n",
    "ğŸ“¦ TÃ©cnicas utilizadas:\n",
    "- Parafraseamento com modelos do Hugging Face\n",
    "- GeraÃ§Ã£o de variaÃ§Ãµes com backtranslation\n",
    "- SubstituiÃ§Ã£o por sinÃ´nimos com spaCy ou NLPAug\n",
    "\n",
    "ğŸ”§ Bibliotecas:\n",
    "- transformers\n",
    "- sentence-transformers\n",
    "- spaCy\n",
    "- nlpaug\n",
    "\n",
    "`Exemplo usando Hugging Face para parafrasear com um modelo prÃ©-treinado`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"Vamsi/T5-paraphraser\")\n",
    "\n",
    "pergunta_original = \"Quais sÃ£o os sabores disponÃ­veis?\"\n",
    "variacoes = paraphraser(f\"paraphrase: {pergunta_original} </s>\", max_length=64, num_return_sequences=5, do_sample=True)\n",
    "\n",
    "for v in variacoes:\n",
    "    print(\"-\", v['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœï¸ 4.1.2 EstratÃ©gia para gerar 20 perguntas com variaÃ§Ãµes de estilo e contexto\n",
    "\n",
    "ğŸ¯ Objetivo: Criar versÃµes formais, informais e regionais da mesma pergunta.\n",
    "\n",
    "ğŸ“¦ TÃ©cnicas utilizadas:\n",
    "- SubstituiÃ§Ã£o lexical com dicionÃ¡rios (gÃ­rias, formalismos)\n",
    "- Reescrita com regras baseadas em regex ou spaCy\n",
    "- Augmenters com estilo (ex: informal/texting)\n",
    "\n",
    "ğŸ”§ Bibliotecas:\n",
    "- nlpaug (contextualWordEmbs + synonym)\n",
    "- pandas + regex + substituiÃ§Ãµes manuais\n",
    "\n",
    "`Exemplo simples: troca informal-formal via regras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"CÃªs tÃªm suco de uva?\"\n",
    "\n",
    "substituicoes = {\n",
    "    \"cÃªs\": \"vocÃªs\",\n",
    "    \"tem\": \"tÃªm\",\n",
    "    \"suco\": \"bebida\",\n",
    "}\n",
    "\n",
    "for g in substituicoes:\n",
    "    pergunta = pergunta.replace(g, substituicoes[g])\n",
    "\n",
    "print(\"Formal:\", pergunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âŒ 4.1.3 EstratÃ©gia para gerar 30 exemplos de erro, exceÃ§Ã£o ou negativa\n",
    "\n",
    "ğŸ¯ Objetivo: Criar perguntas sobre limitaÃ§Ãµes do sistema e situaÃ§Ãµes em que o chatbot deve negar ou informar falha.\n",
    "\n",
    "ğŸ“¦ EstratÃ©gias:\n",
    "- AdiÃ§Ã£o de elementos de falha: \"e se nÃ£o funcionar?\", \"e se acabar o produto?\"\n",
    "- CombinaÃ§Ã£o com palavras-chave negativas: \"nÃ£o\", \"acabou\", \"quebrado\"\n",
    "- GeraÃ§Ã£o por padrÃ£o com templates de exceÃ§Ã£o\n",
    "\n",
    "ğŸ”§ TÃ©cnicas:\n",
    "- Template-based generation com variaÃ§Ãµes programÃ¡ticas\n",
    "- Controle por palavras-chave\n",
    "\n",
    "`Gerador simples de perguntas negativas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "produto = \"limonada\"\n",
    "templates = [\n",
    "    f\"E se acabar a {produto}?\",\n",
    "    f\"O que acontece se a mÃ¡quina travar na hora da {produto}?\",\n",
    "    f\"A mÃ¡quina avisa quando nÃ£o tem mais {produto}?\",\n",
    "    f\"A mÃ¡quina pode errar o sabor da {produto}?\",\n",
    "    f\"Posso pedir reembolso se a {produto} nÃ£o sair?\",\n",
    "]\n",
    "\n",
    "for p in templates:\n",
    "    print(\"-\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ’¡ 4.1.4 EstratÃ©gia para gerar 20 perguntas indiretas, curiosas ou contextuais\n",
    "\n",
    "ğŸ¯ Objetivo: Criar perguntas que nÃ£o pedem algo diretamente, mas fazem referÃªncia contextual, emocional ou hipotÃ©tica.\n",
    "\n",
    "ğŸ“¦ EstratÃ©gias:\n",
    "- GeraÃ§Ã£o com prompts do tipo \"E se...?\", \"SerÃ¡ que...?\", \"Qual Ã© o melhor para...\"\n",
    "- Uso de sentenÃ§as interrogativas abertas\n",
    "- InspiraÃ§Ã£o em FAQs, experiÃªncias de usuÃ¡rios reais\n",
    "\n",
    "ğŸ”§ TÃ©cnicas:\n",
    "- Manual com apoio de GPT ou modelo instruÃ­do\n",
    "- CriaÃ§Ã£o de padrÃµes com linguagem subjetiva\n",
    "\n",
    "ğŸ”§ Bibliotecas:\n",
    "- transformers (para gerar exemplos a partir de poucos prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", max_length=60)\n",
    "\n",
    "prompt = \"E se eu esquecer a carteira na hora de pagar?\"\n",
    "resposta = generator(prompt, do_sample=True, top_p=0.9, temperature=0.7, num_return_sequences=3)\n",
    "\n",
    "for r in resposta:\n",
    "    print(\"-\", r[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
