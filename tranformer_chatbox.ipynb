{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñüí¨ Chatbot inteligente para gera√ß√£o de respostas em linguagem pt-br\n",
    "\n",
    "Este projeto utiliza o modelo **PTT5 (Portuguese T5)**, uma adapta√ß√£o do modelo **T5 (Text-to-Text Transfer Transformer)** para a l√≠ngua portuguesa.\n",
    "\n",
    "- üìê **Arquitetura:** baseada no Transformer, com codificador e decodificador (encoder-decoder).\n",
    "- üß† **Capacidade:** compreende instru√ß√µes em linguagem natural e gera respostas coerentes e contextualizadas.\n",
    "- üìö **Aprendizado:** usa fine-tuning sobre dados em portugu√™s via aprendizado por transfer√™ncia.\n",
    "- üõ†Ô∏è **Base:** modelo `unicamp-dl/ptt5-base-portuguese-vocab`.\n",
    "\n",
    "\n",
    "> ‚ö†Ô∏è dataset.csv, requirements.txt inclu√≠das no reposit√≥rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß∞ Parte 1: Ambiente e depend√™ncias\n",
    "\n",
    "Nesta se√ß√£o, definimos o ambiente necess√°rio para executar o projeto, garantindo que todas as bibliotecas estejam corretamente instaladas e compat√≠veis com o modelo utilizado.\n",
    "\n",
    "As etapas abaixo cobrem:\n",
    "\n",
    "- Especifica√ß√£o da vers√£o do Python e sistema operacional\n",
    "- Organiza√ß√£o do ambiente virtual (Conda recomendado)\n",
    "- Instala√ß√£o das depend√™ncias via `pip` ou `requirements.txt`\n",
    "- Registro das bibliotecas utilizadas no desenvolvimento\n",
    "\n",
    "> ‚ö†Ô∏è Ter um ambiente reprodut√≠vel √© essencial para evitar conflitos de vers√£o e garantir que o modelo funcione como esperado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öôÔ∏è 1.1: Ambiente e depend√™ncias utilizadas\n",
    "\n",
    "O modelo foi treinado e executado em um ambiente com a seguinte configura√ß√£o:\n",
    "\n",
    "- üêç **Python**: 3.9.x (via Anaconda) \n",
    "- üíª **Sistema Operacional**: Windows (compat√≠vel tamb√©m com Linux)  \n",
    "- üß™ **Ambiente virtual**: criado com o **Conda**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì¶ 1.2: Bibliotecas essenciais\n",
    "\n",
    "!pip install transformers==4.37.2\n",
    "!pip install datasets==2.16.1\n",
    "!pip install pandas==2.1.4\n",
    "!pip install torch==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üóÉÔ∏è 1.3: Instala√ß√£o das depend√™ncias\n",
    "\n",
    "Caso queira utilizar todas as bibliotecas que foram utilizadas utilize o arquivo `requirements.txt`. Para instalar, ative o ambiente Conda desejado e execute:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è‚Äç‚ôÇÔ∏è Parte 2: Treinamento do modelo PTT5 com dados personalizados\n",
    "\n",
    "Nesta se√ß√£o, executamos todas as etapas relacionadas ao **processo de fine-tuning do modelo PTT5** em portugu√™s, utilizando um dataset customizado.\n",
    "\n",
    "As etapas abaixo cobrem:\n",
    "\n",
    "- Importa√ß√£o de bibliotecas\n",
    "- Carregamento e prepara√ß√£o do modelo\n",
    "- Processamento e tokeniza√ß√£o dos dados\n",
    "- Defini√ß√£o de argumentos de treinamento\n",
    "- Execu√ß√£o do treinamento com monitoramento de loss\n",
    "- Salvamento do modelo final e uso para infer√™ncia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíª Etapa 2.1: Importa√ß√£o das bibliotecas\n",
    "\n",
    "        Importamos os m√≥dulos essenciais para manipular dados, preparar o modelo e realizar o treinamento com Transformers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìò Etapa 2.2: Callback para monitorar a perda por √©poca\n",
    "\n",
    "        Esta fun√ß√£o imprime a loss ao final de cada √©poca durante o treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLossPrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\nüìò [√âpoca {int(state.epoch)}] Loss de Treinamento: {state.log_history[-1]['loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì• Etapa 2.3: Carregando o modelo PTT5 e o tokenizer\n",
    "\n",
    "        Utilizamos a vers√£o base do modelo T5 treinado em portugu√™s pela Unicamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unicamp-dl/ptt5-base-portuguese-vocab\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì• Etapa 2.4: Carregando e preparando o dataset CSV\n",
    "\n",
    "        Unificamos as colunas do dataset em uma entrada textual formatada para o modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df[\"input_text\"] = \"t√≥pico: \" + df[\"topic\"] + \" | instru√ß√£o: \" + df[\"instruction\"]\n",
    "df[\"target_text\"] = df[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Etapa 2.5: Transforma√ß√£o do DataFrame em um objeto Dataset \n",
    "\n",
    "        Transformamos o DataFrame em um objeto Dataset para integra√ß√£o com o `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df[[\"input_text\", \"target_text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß© Etapa 2.6: Tokeniza√ß√£o dos dados\n",
    "\n",
    "        Aplicamos truncamento, padding e preparamos os pares entrada/sa√≠da para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    input_enc = tokenizer(batch[\"input_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    target_enc = tokenizer(batch[\"target_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìò Etapa 2.7: Configura√ß√£o dos hiperpar√¢metros de treinamento\n",
    "\n",
    "        Definimos estrat√©gia de salvamento, taxa de aprendizado, uso de FP16 e n√∫mero de √©pocas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ptt5-modelo-treinado\",       # Pasta onde os checkpoints do modelo ser√£o salvos\n",
    "    overwrite_output_dir=True,                 # Sobrescreve a pasta de sa√≠da se ela j√° existir\n",
    "    per_device_train_batch_size=30,            # Tamanho do batch por dispositivo (ex: GPU) durante o treinamento\n",
    "    gradient_accumulation_steps=2,             # Acumula gradientes por 2 batches antes de atualizar os pesos (simula batch_size=60)\n",
    "    learning_rate=3e-4,                        # Taxa de aprendizado inicial (valor relativamente alto, ideal para fine-tuning)\n",
    "    num_train_epochs=30,                       # N√∫mero total de √©pocas de treinamento (passadas completas pelo dataset)\n",
    "    save_strategy=\"epoch\",                     # Salva um checkpoint do modelo ao final de cada √©poca\n",
    "    weight_decay=0.005,                        # Taxa de decaimento dos pesos (regulariza√ß√£o para evitar overfitting)\n",
    "    warmup_steps=100,                          # N√∫mero de steps com aprendizado mais suave no in√≠cio (warm-up)\n",
    "    logging_steps=10,                          # Frequ√™ncia (em steps) com que as m√©tricas de treino ser√£o logadas no console\n",
    "    fp16=torch.cuda.is_available(),            # Ativa treinamento em precis√£o mista (FP16) se houver GPU compat√≠vel (mais r√°pido e leve)\n",
    "    report_to=\"none\",                          # Desativa integra√ß√£o com sistemas de logging externos (ex: TensorBoard, WandB)\n",
    "    save_total_limit=3                         # Mant√©m no m√°ximo 3 checkpoints salvos no disco (os mais recentes)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öôÔ∏è Etapa 2.8: Inicializa√ß√£o do Trainer\n",
    "\n",
    "        Instanciamos o `Trainer` com o modelo, os dados tokenizados e o callback para logging da loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                               # Modelo PTT5 carregado e pronto para ser treinado\n",
    "    args=training_args,                        # Conjunto de argumentos de treinamento definidos anteriormente (TrainingArguments)\n",
    "    train_dataset=tokenized_dataset,           # Dataset j√° tokenizado que ser√° usado para o treinamento\n",
    "    callbacks=[EpochLossPrinterCallback()]     # Lista de callbacks personalizados; neste caso, imprime a loss ao final de cada √©poca\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ Etapa 2.9: In√≠cio do treinamento\n",
    "\n",
    "        Executamos o fine-tuning do modelo com os dados fornecidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíæ Etapa 2.10: Salvando o modelo e o tokenizer treinados\n",
    "\n",
    "        Ap√≥s o treinamento, salvamos o modelo fine-tunado e seu tokenizer para uso posterior em infer√™ncia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./ptt5-modelo-final\")\n",
    "tokenizer.save_pretrained(\"./ptt5-modelo-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üí¨ Parte 3: Uso do modelo treinado para gera√ß√£o de respostas\n",
    "\n",
    "Com o modelo PTT5 j√° treinado e salvo, esta se√ß√£o aborda as etapas necess√°rias para utiliz√°-lo em tempo de execu√ß√£o.\n",
    "\n",
    "Aqui, veremos:\n",
    "\n",
    "- Como **carregar o modelo salvo** e seu tokenizer\n",
    "- Definir uma fun√ß√£o `responder()` que gere respostas a partir de perguntas em linguagem natural\n",
    "- Realizar **testes pr√°ticos com perguntas reais** simulando um chatbot\n",
    "- Explorar diferentes configura√ß√µes de gera√ß√£o, como `temperature`, `top_k` e `top_p`\n",
    "\n",
    "> üìå Esta se√ß√£o √© fundamental para validar a performance pr√°tica do modelo ap√≥s o fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîÅ Etapa 3.1: Carregando o modelo final para uso\n",
    "\n",
    "        Agora vamos utilizar o modelo PTT5 fine-tunado para responder perguntas reais em portugu√™s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Caminho para o modelo salvo\n",
    "model_dir = \"./ptt5-modelo-final\"\n",
    "\n",
    "# Carregando o tokenizer e o modelo\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìò Etapa 3.2: Fun√ß√£o para gera√ß√£o de respostas\n",
    "\n",
    "        Criamos uma fun√ß√£o `responder()` que recebe uma instru√ß√£o e retorna uma resposta gerada pelo modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def responder(pergunta):\n",
    "    # Concatena o prefixo \"instru√ß√£o: \" com a pergunta do usu√°rio (formato usado no treinamento do modelo)\n",
    "    entrada = \"instru√ß√£o: \" + pergunta\n",
    "\n",
    "    # Tokeniza a entrada textual para o formato aceito pelo modelo\n",
    "    inputs = tokenizer(\n",
    "        entrada,                 # Texto de entrada para o modelo\n",
    "        return_tensors=\"pt\",     # Retorna tensores do PyTorch\n",
    "        truncation=True,         # Trunca o texto se ultrapassar o limite m√°ximo\n",
    "        padding=True,            # Aplica padding para atingir o tamanho fixo\n",
    "        max_length=128           # Define o tamanho m√°ximo da sequ√™ncia de entrada\n",
    "    )\n",
    "\n",
    "    # Gera a sa√≠da textual usando o modelo treinado\n",
    "    outputs = model.generate(\n",
    "        **inputs,                # Passa os tensores tokenizados como entrada\n",
    "        max_length=64,           # Limita o tamanho da resposta gerada\n",
    "        do_sample=True,          # Ativa amostragem aleat√≥ria (mais criativo)\n",
    "        top_k=50,                # Considera apenas os 50 tokens mais prov√°veis (Top-K sampling)\n",
    "        top_p=0.95,              # Aplica nucleus sampling (Top-P), acumulando at√© 95% de probabilidade\n",
    "        temperature=0.7,         # Controla a aleatoriedade (quanto menor, mais conservador)\n",
    "        repetition_penalty=1.2,  # Penaliza repeti√ß√µes para evitar respostas redundantes\n",
    "        num_return_sequences=1   # Gera apenas uma resposta\n",
    "    )\n",
    "\n",
    "    # Decodifica a resposta gerada de volta para string leg√≠vel (removendo tokens especiais)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí¨ Etapa 3.3: Fazendo perguntas ao modelo treinado\n",
    "        \n",
    "        Abaixo, alguns exemplos reais testando o modelo com instru√ß√µes t√≠picas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responder(\"A m√°quina aceita cart√£o?\"))\n",
    "print(responder(\"Tem coca-cola?\"))\n",
    "print(responder(\"Se um turista usar a m√°quina, ele vai conseguir entender?\"))\n",
    "print(responder(\"Posso pagar com boleto?\"))\n",
    "print(responder(\"Quais s√£o a forma de pagamento?\"))\n",
    "print(responder(\"Tem bebida com g√°s?\"))\n",
    "print(responder(\"Tem op√ß√£o de refrigerante sem cafe√≠na?\"))\n",
    "print(responder(\"O refrigerante n√£o sai da m√°quina, como devo proceder?\"))\n",
    "print(responder(\"Qual telefone de suporte?\"))\n",
    "print(responder(\"Voc√™ tem laranja?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÇÔ∏è Parte 4: Constru√ß√£o do dataset\n",
    "\n",
    "Nesta se√ß√£o, explicamos como foi constru√≠do o dataset utilizado no fine-tuning do modelo PTT5, focado na gera√ß√£o de respostas em portugu√™s com base em instru√ß√µes espec√≠ficas.\n",
    "\n",
    "As etapas abaixo cobrem:\n",
    "\n",
    "- Estrutura das colunas (`topic`, `instruction`, `output`)\n",
    "- Estrat√©gia de formula√ß√£o das instru√ß√µes e respostas\n",
    "- Gera√ß√£o e organiza√ß√£o dos exemplos em CSV\n",
    "- Objetivo dos dados: simular intera√ß√µes reais com um chatbot\n",
    "\n",
    "> üß† A qualidade e variedade do dataset s√£o fatores decisivos para o desempenho do modelo. Neste projeto, priorizamos instru√ß√µes curtas, diretas e contextualizadas, refletindo situa√ß√µes reais de atendimento automatizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ 4.1: Estrat√©gia de constru√ß√£o dos exemplos\n",
    "\n",
    "> A partir de 5 perguntas humanas por t√≥pico, voc√™ poder√° gerar as 100 por t√≥pico\n",
    "\n",
    "Para garantir a robustez e a naturalidade das respostas geradas pelo modelo, cada t√≥pico do dataset foi estruturado com 100 exemplos balanceados em quatro categorias distintas:\n",
    "\n",
    "- **30 perguntas variadas com estrutura direta**, explorando diferentes formas de expressar a mesma inten√ß√£o;\n",
    "- **20 perguntas com varia√ß√µes de estilo e contexto**, incluindo registros formais, informais e constru√ß√µes regionais;\n",
    "- **30 casos de erro, exce√ß√£o ou negativa**, representando limita√ß√µes reais do sistema (ex: produto indispon√≠vel, recurso inexistente);\n",
    "- **20 perguntas indiretas, curiosidades ou detalhes contextuais**, que exigem infer√™ncia sem√¢ntica ou compreens√£o mais sutil.\n",
    "\n",
    "O conjunto total abrange **11 t√≥picos espec√≠ficos** relacionados ao funcionamento da m√°quina de venda: `sabores`, `acessibilidade`, `hor√°rio de funcionamento`, `idioma`, `promo√ß√µes`, `contato`, `reembolso`, `falhas`, `uso da m√°quina`, `produtos` e `pagamento`.\n",
    "\n",
    "Essa abordagem visa simular intera√ß√µes aut√™nticas com usu√°rios em cen√°rios reais, aumentando a capacidade do modelo de generalizar, recusar adequadamente e manter coer√™ncia mesmo diante de instru√ß√µes incomuns ou incompletas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå 4.1.1 Estrat√©gia para gerar 30 perguntas variadas com estrutura diferente\n",
    "\n",
    "üéØ Objetivo: Gerar varia√ß√µes de forma (estrutura gramatical) mantendo a mesma inten√ß√£o da pergunta.\n",
    "\n",
    "üì¶ T√©cnicas utilizadas:\n",
    "- Parafraseamento com modelos do Hugging Face\n",
    "- Gera√ß√£o de varia√ß√µes com backtranslation\n",
    "- Substitui√ß√£o por sin√¥nimos com spaCy ou NLPAug\n",
    "\n",
    "üîß Bibliotecas:\n",
    "- transformers\n",
    "- sentence-transformers\n",
    "- spaCy\n",
    "- nlpaug\n",
    "\n",
    "`Exemplo usando Hugging Face para parafrasear com um modelo pr√©-treinado`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"Vamsi/T5-paraphraser\")\n",
    "\n",
    "pergunta_original = \"Quais s√£o os sabores dispon√≠veis?\"\n",
    "variacoes = paraphraser(f\"paraphrase: {pergunta_original} </s>\", max_length=64, num_return_sequences=5, do_sample=True)\n",
    "\n",
    "for v in variacoes:\n",
    "    print(\"-\", v['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úçÔ∏è 4.1.2 Estrat√©gia para gerar 20 perguntas com varia√ß√µes de estilo e contexto\n",
    "\n",
    "üéØ Objetivo: Criar vers√µes formais, informais e regionais da mesma pergunta.\n",
    "\n",
    "üì¶ T√©cnicas utilizadas:\n",
    "- Substitui√ß√£o lexical com dicion√°rios (g√≠rias, formalismos)\n",
    "- Reescrita com regras baseadas em regex ou spaCy\n",
    "- Augmenters com estilo (ex: informal/texting)\n",
    "\n",
    "üîß Bibliotecas:\n",
    "- nlpaug (contextualWordEmbs + synonym)\n",
    "- pandas + regex + substitui√ß√µes manuais\n",
    "\n",
    "`Exemplo simples: troca informal-formal via regras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"C√™s t√™m suco de uva?\"\n",
    "\n",
    "substituicoes = {\n",
    "    \"c√™s\": \"voc√™s\",\n",
    "    \"tem\": \"t√™m\",\n",
    "    \"suco\": \"bebida\",\n",
    "}\n",
    "\n",
    "for g in substituicoes:\n",
    "    pergunta = pergunta.replace(g, substituicoes[g])\n",
    "\n",
    "print(\"Formal:\", pergunta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùå 4.1.3 Estrat√©gia para gerar 30 exemplos de erro, exce√ß√£o ou negativa\n",
    "\n",
    "üéØ Objetivo: Criar perguntas sobre limita√ß√µes do sistema e situa√ß√µes em que o chatbot deve negar ou informar falha.\n",
    "\n",
    "üì¶ Estrat√©gias:\n",
    "- Adi√ß√£o de elementos de falha: \"e se n√£o funcionar?\", \"e se acabar o produto?\"\n",
    "- Combina√ß√£o com palavras-chave negativas: \"n√£o\", \"acabou\", \"quebrado\"\n",
    "- Gera√ß√£o por padr√£o com templates de exce√ß√£o\n",
    "\n",
    "üîß T√©cnicas:\n",
    "- Template-based generation com varia√ß√µes program√°ticas\n",
    "- Controle por palavras-chave\n",
    "\n",
    "`Gerador simples de perguntas negativas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "produto = \"limonada\"\n",
    "templates = [\n",
    "    f\"E se acabar a {produto}?\",\n",
    "    f\"O que acontece se a m√°quina travar na hora da {produto}?\",\n",
    "    f\"A m√°quina avisa quando n√£o tem mais {produto}?\",\n",
    "    f\"A m√°quina pode errar o sabor da {produto}?\",\n",
    "    f\"Posso pedir reembolso se a {produto} n√£o sair?\",\n",
    "]\n",
    "\n",
    "for p in templates:\n",
    "    print(\"-\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° 4.1.4 Estrat√©gia para gerar 20 perguntas indiretas, curiosas ou contextuais\n",
    "\n",
    "üéØ Objetivo: Criar perguntas que n√£o pedem algo diretamente, mas fazem refer√™ncia contextual, emocional ou hipot√©tica.\n",
    "\n",
    "üì¶ Estrat√©gias:\n",
    "- Gera√ß√£o com prompts do tipo \"E se...?\", \"Ser√° que...?\", \"Qual √© o melhor para...\"\n",
    "- Uso de senten√ßas interrogativas abertas\n",
    "- Inspira√ß√£o em FAQs, experi√™ncias de usu√°rios reais\n",
    "\n",
    "üîß T√©cnicas:\n",
    "- Manual com apoio de GPT ou modelo instru√≠do\n",
    "- Cria√ß√£o de padr√µes com linguagem subjetiva\n",
    "\n",
    "üîß Bibliotecas:\n",
    "- transformers (para gerar exemplos a partir de poucos prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", max_length=60)\n",
    "\n",
    "prompt = \"E se eu esquecer a carteira na hora de pagar?\"\n",
    "resposta = generator(prompt, do_sample=True, top_p=0.9, temperature=0.7, num_return_sequences=3)\n",
    "\n",
    "for r in resposta:\n",
    "    print(\"-\", r[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
